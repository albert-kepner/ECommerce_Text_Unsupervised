{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "import warnings \n",
    "## warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0b3b8",
   "metadata": {},
   "source": [
    "## Data Loading and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2900d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data set from a CSV file\n",
    "## There is no header row in the input file.\n",
    "## The first column is the label or Product_Category, and the 2nd column is\n",
    "## the product and description as a string.\n",
    "df = pd.read_csv('./data/ecommerceDataset.csv',header=None)\n",
    "\n",
    "df.columns=['product_category','raw_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3571bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the occurences of each product category\n",
    "df['product_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6d5fc",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The data has no missing values, but the product descriptions are free-form text. We plan to use the GloVe pre-trained word\n",
    "vectors to encode all the words we can match from the product descriptions. But the first steps are to convert all the text to lower case, remove all punctuation and non-alphabetic characters, and remove English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98328e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_non_alpha = re.compile(r'[^-a-z]+')\n",
    "\n",
    "def clean_words(text):\n",
    "    \"\"\"\n",
    "    1) Change all text to lower case.\n",
    "    2) Substitute space for all non-alphabetic characters (allow hyphen to remain)\n",
    "    3) Split into word tokens and drop English stop words (using ENGLISH_STOP_WORDS from sklearn.feature_extraction.text\n",
    "    4) Remove single letter tokens\n",
    "    4) Return a string which concatenates all remaining words in each text.\n",
    "    \"\"\"\n",
    "    text2 = match_non_alpha.sub(r' ',text.lower())\n",
    "    tokens = text2.split(' ')\n",
    "    cleaned = []\n",
    "    for token in tokens:\n",
    "        ## Also remove single letter tokens, \n",
    "        ## Also remove English stop words from scikit-learn feature_extraction\n",
    "        if len(token) > 1 and token not in ENGLISH_STOP_WORDS:\n",
    "            cleaned.append(token)\n",
    "    clean_text = ' '.join(cleaned)\n",
    "    return clean_text\n",
    "        \n",
    "## function to count words in text\n",
    "def count_words(text):\n",
    "    return len (text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['raw_text'].apply(clean_words)\n",
    "df['word_counts'] = df['cleaned_text'].apply(count_words)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6af2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_counts'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It turns out that after removing stop words and puctuation, etc, \n",
    "## A few of the cleaned_text values are empty. We will drop these rows\n",
    "## from the data frame.\n",
    "\n",
    "df[df['cleaned_text']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['cleaned_text']!='']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76792038",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list of all (cleaned) words used in the product descriptions\n",
    "## and identify the 50 most frequently occuring words\n",
    "corpus = list(df['cleaned_text'])\n",
    "\n",
    "all_words = []\n",
    "for text in corpus:\n",
    "    all_words.extend(text.split(' '))\n",
    "    \n",
    "print(f'Total Words in the corpus = {len(all_words)}')\n",
    "\n",
    "all_word_series = pd.Series(all_words)\n",
    "display(all_word_series.describe())\n",
    "    \n",
    "top_50 = pd.DataFrame(\n",
    "    Counter(all_words).most_common(50),\n",
    "    columns=['word', 'frequency']\n",
    ")\n",
    "\n",
    "top_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae71350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart for the top 50 most frequently occuring words\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "g = sns.barplot(\n",
    "    x='word',\n",
    "    y='frequency',\n",
    "    data=top_50,\n",
    ")\n",
    "\n",
    "g.set_xticklabels(\n",
    "    g.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Words', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Top 50 Words', fontsize=17)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4240ed92",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "We plan to use supervised LSTM model for classification of the 4 product categories.\n",
    "Therefore we need to reserve some of the data for out-of-sample testing. We will hold out 20% for test. We will encode the cleaned text from the product descriptions as sequences of GloVe vectors (300 floats for each word).\n",
    "\n",
    "We are using GloVe pre-trained word vectors from this website: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Reference: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.\n",
    "\n",
    "GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "The GloVe vectors are trained on a large corpus of text, in such a way that word with similar meaning have similar directions in the vector space.\n",
    "\n",
    "We will be matching all the words in our product descriptions with a dictionary of the Glove vectors and so converting a sequence of words into a sequence of these vectors. Words not in the dictionary will be dropped but most commonly used words should be found. The dictionary we are using has a 1.9 million token vocabulary. To build an LSTM model we need all the word sequences to be of a uniform length. Therefore we will be truncating or padding all the word sequences to a uniform length of 150 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6210fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are reserving 20 % of the data for testing\n",
    "train, test = train_test_split(df, train_size=0.8, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c62f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to Load the GloVe dictionary of word embeddings.\n",
    "## Using GloVe pre-trained word vectors from this website: https://nlp.stanford.edu/projects/glove/\n",
    "## \n",
    "## Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. \n",
    "## GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/pubs/glove.pdf\n",
    "##\n",
    "## This particular set of word vectors is described as:\n",
    "## Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\n",
    "## \n",
    "\n",
    "def get_glove_vectors(filename=\"data/glove.42B.300d.txt\"):\n",
    "    ## function from \n",
    "    ## https://campus.datacamp.com/courses/recurrent-neural-networks-for-language-modeling-in-python/rnn-architecture?ex=7\n",
    "    # Get all word vectors from pre-trained model\n",
    "    glove_vector_dict = {}\n",
    "    with open(filename, encoding=\"UTF-8\") as f:\n",
    "        i = 0;\n",
    "        for line in f:\n",
    "            i = i + 1\n",
    "            values = line.split()\n",
    "            try:\n",
    "                word = values[0]\n",
    "                coefs = values[1:]\n",
    "                glove_vector_dict[word] = np.asarray(coefs, dtype='float32')\n",
    "            except Exception as inst:\n",
    "                print(f'error on line {i} {type(inst)}')\n",
    "                print(line)\n",
    "    return glove_vector_dict\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "glove_vector_dict = get_glove_vectors()\n",
    "\n",
    "end = time.time()\n",
    "print(f'elapsed seconds = {end - start}')\n",
    "type(glove_vector_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62689f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_word_embeddings(documents, pad_to=150):\n",
    "    ## We plan to replace all the words in the documents\n",
    "    ## with embeddings from the GloVe dictionary, skipping\n",
    "    ## any words not found, and also padding the sequence \n",
    "    ## of embeddings to a fixed length.\n",
    "    \n",
    "    ## If none of the words match for a given document we will substitute\n",
    "    ## a with place holder vector of one word, \"neutral\".\n",
    "    d = glove_vector_dict\n",
    "    neutral = d[\"neutral\"]\n",
    "    placeHolder = np.array([neutral])\n",
    "    padNeutral = pad_sequences(placeHolder.T, pad_to, dtype='float32')\n",
    "    outer = []\n",
    "    for doc in documents:\n",
    "        enc_list = []\n",
    "        for word in doc.split(' '):\n",
    "            if(type(d.get(word)) is np.ndarray):\n",
    "                enc_list.append(d.get(word))\n",
    "        if(len(enc_list) > 0):\n",
    "            enc_array = np.array(enc_list)\n",
    "            pad = pad_sequences(enc_array.T, pad_to, dtype='float32')\n",
    "            outer.append(pad.T)\n",
    "        else:\n",
    "            outer.append(padNeutral.T)\n",
    "    return np.array(outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train = glove_word_embeddings(train['cleaned_text'])\n",
    "display(X_train.shape)\n",
    "labelEncoderOnehot = LabelBinarizer()\n",
    "y_train = labelEncoderOnehot.fit_transform(train['product_category'])\n",
    "display(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.2\n",
    "UNITS_PER_LAYER = 64\n",
    "\n",
    "## Try switching to a Bidirectional LSTM model, as in this example\n",
    "## https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(150, 300) )\n",
    "x = layers.Bidirectional(LSTM(units=UNITS_PER_LAYER, return_sequences=True, dropout=DROPOUT))(inputs)\n",
    "x = layers.Bidirectional(LSTM(units=UNITS_PER_LAYER, return_sequences=True, dropout=DROPOUT))(x)\n",
    "x = layers.Bidirectional(LSTM(units=UNITS_PER_LAYER, return_sequences=False, dropout=DROPOUT))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(4,  activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "file_name = 'weights_{epoch:03d}_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "checkpoint_filepath = os.path.join('.', 'SAVE_MODELS', file_name)\n",
    "\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, restore_best_weights=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=1, \n",
    "                    epochs=100, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[earlyStopping,modelCheckpoint]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a4086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
